Artificial Inelligence
	Desarrollo software & AI
	https://www.cio.com/article/3437436/rethinking-software-development-in-the-ai-era.html?nsdr=true

$ wget https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz
$ tar -xvzf spark-3.3.2-bin-hadoop3.tgz
                                                                                     
$ vim ~/.bashrc
export SPARK_HOME=/path/to/spark
export PY4J_PATH=/path/to/spark/lib/py4j-<version>.jar
export PATH=$SPARK_HOME/bin:$PATH
$ source ~/.bashrc
                                                                                     
                                                                                     
$ python3 -m venv .venv
$ source .venv/bin/activate
$ .venv\Scripts\activate
$ pip install pyspark==3.3.2
$ pip install pyspark[sql]
$ pip install pyspark[pandas_on_spark]

Verify installation:
$ python -c "import pyspark; print(pyspark.__version__)"
                                                                                     
C:\Users\v075756\Documents\txt\lorem.txt
logFile = "file:///home/hadoop/spark-2.1.0-bin-hadoop2.7/README.md"
                                                                                     
logFile = "file:///C:/Users/v075756/Documents/txt/lorem.txt"
logData = sc.textFile(logFile).cache()
numAs = logData.filter(lambda s: 'a' in s).count()
numBs = logData.filter(lambda s: 'b' in s).count()
print("Lines with a: %i, lines with b: %i" % (numAs, numBs))

from pyspark import SparkContext

logFile = "file:///C:/Users/v075756/Documents/txt/lorem.txt"
sc = SparkContext("local", "First App")
logData = sc.textFile(logFile).cache()
numAs = logData.filter(lambda s: 'a' in s).count()
numBs = logData.filter(lambda s: 'b' in s).count()
print(f"Lines with a: {numAs}, lines with b: {numBs}")
# print ("Lines with a: %i, lines with b: %i" % (numAs, numBs)

----

from pyspark.sql import SparkSession
from pyspark.sql.functions import concat_ws, col

# Initialize Spark session
spark = SparkSession.builder \
 .appName("CSV Loader") \
 .getOrCreate()

df = spark.read.csv('cars.csv', header=True, sep=";")
# df.show(5)
# Add new transformation column by concatenating Car, Model, and Origin
df = df.withColumn("transformation", 
                  concat_ws(" - ", col("Car"), col("Model"), col("Origin")))

# Show the first 5 rows with the new column
df.show(5, truncate=False)
